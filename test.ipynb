{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30de7ff6",
   "metadata": {},
   "source": [
    "Importing Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0004e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeevin/miniconda3/envs/paynet-test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeevin/.cache/kagglehub/datasets/jinquan/cc-sample-data/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/21 00:59:04 WARN Utils: Your hostname, jeevin, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/21 00:59:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 00:59:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from kagglehub import dataset_download\n",
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, split, when, lower, round as sp_round, from_utc_timestamp, date_format, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, LongType\n",
    "import re\n",
    "\n",
    "path: str = dataset_download(\"jinquan/cc-sample-data\")\n",
    "\n",
    "print(path)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"payNet\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67be3f",
   "metadata": {},
   "source": [
    "Load JSON and clean up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95900d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+-------------------+--------------------+-------------+------+--------------------+----------------+------+--------------------+--------------------+-----+-----+-------+---------+--------+--------------------+----------+--------------------+---------+----------+--------+-------------+----------------------+----------------+-----------+\n",
      "|Unnamed: 0|trans_date_trans_time|             cc_num|            merchant|     category|   amt|               first|            last|gender|              street|                city|state|  zip|    lat|     long|city_pop|                 job|       dob|           trans_num|merch_lat|merch_long|is_fraud|merch_zipcode|merch_last_update_time|  merch_eff_time|     cc_bic|\n",
      "+----------+---------------------+-------------------+--------------------+-------------+------+--------------------+----------------+------+--------------------+--------------------+-----+-----+-------+---------+--------+--------------------+----------+--------------------+---------+----------+--------+-------------+----------------------+----------------+-----------+\n",
      "|         0|  2019-01-01 00:00:18|   2703186189652095|fraud_Rippin, Kub...|     misc_net|  4.97|            Jennifer|     Banks,eeeee|     F|      561 Perry Cove|      Moravian Falls|   NC|28654|36.0788| -81.1781|    3495|Psychologist, cou...|1988-03-09|0b242abb623afc578...| 36.01129| -82.04832|       0|        28705|         1325376018666|1325376018798532|CITIUS33CHI|\n",
      "|         1|  2019-01-01 00:00:44|       630423337322|fraud_Heller, Gut...|  grocery_pos|107.23|           Stephanie|      Gill,eeeee|     F|43039 Riley Green...|              Orient|   WA|99160|48.8878|-118.2105|     149|Special education...|1978-06-21|1f76529f857473494...|49.159046|-118.18646|       0|         NULL|          132537604479|1325376044867960|   ADMDUS41|\n",
      "|         2|  2019-01-01 00:00:51|     38859492057661|fraud_Lind-Buckridge|entertainment|220.11|      Edward@Sanchez|            NULL|     M|594 White Dale Su...|          Malad City|   ID|83252|42.1808| -112.262|    4154|Nature conservati...|1962-01-19|a1a22d70485983eac...|43.150703|-112.15448|       0|        83236|         1325376051286|1325376051506840|       NULL|\n",
      "|         3|  2019-01-01 00:01:16|   3534093764340240|fraud_Kutch, Herm...|gas_transport|  45.0|        Jeremy/White|               !|     M|9443 Cynthia Cour...|             Boulder|   MT|59632|46.2306|-112.1138|    1939|     Patent attorney|1967-01-12|6b849c168bdad6f86...|47.034332|-112.56107|       0|         NULL|         1325376076365|1325376076794698|DEUTUS33TRF|\n",
      "|         4|  2019-01-01 00:03:06|    375534208663984| fraud_Keeling-Crist|     misc_pos| 41.96|        Tyler@Garcia|            NULL|     M|    408 Bradley Rest|            Doe Hill|   VA|24433|38.4207| -79.4629|      99|Dance movement ps...|1986-03-28|a41d7549acf907893...|   38.675| -78.63246|       0|        22844|          132537618681|1325376186746376|   APBCUS61|\n",
      "|         5|  2019-01-01 00:04:08|   4767265376804500|fraud_Stroman, Hu...|gas_transport| 94.63|            Jennifer|    Conner,eeeee|     F|   4655 David Island|              Dublin|   PA|18917| 40.375| -75.2045|    2158|   Transport planner|1961-06-19|189a841a0a8ba0305...| 40.65338|-76.152664|       0|        17972|         1325376248483|1325376248271406|   APBCUS61|\n",
      "|         6|  2019-01-01 00:04:42|     30074693890476|fraud_Rowe-Vander...|  grocery_net| 44.54|              Kelsey|, Richards NOOOO|     F|889 Sarah Station...|             Holcomb|   KS|67851|37.9931|-100.9893|    2691|     Arboriculturist|1993-08-16|83ec1cc84142af6e2...|37.162704|-100.15337|       0|         NULL|         1325376282247|1325376282274130|   APBCUS61|\n",
      "|         7|  2019-01-01 00:05:08|   6011360759745864|fraud_Corwin-Collins|gas_transport| 71.65|              Steven|        Williams|     M|231 Flores Pass S...|            Edinburg|   VA|22824|38.8432| -78.6003|    6018|Designer, multimedia|1947-08-21|6d294ed2cc447d2c7...| 38.94809|  -78.5403|       0|        22644|         1325376308152|1325376308837349|       NULL|\n",
      "|         8|  2019-01-01 00:05:18|   4922710831011201|    fraud_Herzog Ltd|     misc_pos|  4.27|             Heather|   , Chase NOOOO|     F|6888 Hicks Stream...|               Manor|   PA|15665|40.3359| -79.6607|    1472|Public affairs co...|1941-03-07|fc28024ce480f8ef2...|40.351814|-79.958145|       0|        15236|         1325376318278|1325376318245892|   ACEEUS31|\n",
      "|         9|  2019-01-01 00:06:01|   2720830304681674|fraud_Schoen, Kup...|  grocery_pos|198.39|     Melissa@Aguilar|            NULL|     F|21326 Taylor Squa...|         Clarksville|   TN|37040| 36.522|  -87.349|  151785|         Pathologist|1974-03-28|3b9014ea8fb80bd65...|  37.1792| -87.48538|       0|        42442|         1325376361857|1325376361965234|DEUTUS33TRF|\n",
      "|        10|  2019-01-01 00:06:23|      4642894980163|fraud_Rutherford-...|  grocery_pos| 24.74|     Eddie|Mendez!!!|            NULL|     M|1831 Faith View S...|            Clarinda|   IA|51632|40.7491|  -95.038|    7297|          IT trainer|1990-07-13|d71c95ab6b7356dd7...| 40.27589| -96.01155|       0|        68348|         1325376383455|1325376383967287|       NULL|\n",
      "|        11|  2019-01-01 00:06:53|    377234009633447|fraud_Kerluke-Abs...| shopping_net|  7.77|   Theresa@Blackwell|            NULL|     F|43576 Kristina Is...| Shenandoah Junction|   WV|25442|39.3716| -77.8229|    1925|   Systems developer|1966-02-14|3c74776e558f1499a...|40.103867| -78.62446|       0|        15554|         1325376413859|1325376413912233|DEUTUS33TRF|\n",
      "|        12|  2019-01-01 00:06:56|    180042946491150|   fraud_Lockman Ltd|  grocery_pos| 71.22|   Charles|Robles!!!|            NULL|     M|    3337 Lisa Divide|    Saint Petersburg|   FL|33710|27.7898| -82.7243|  341043|      Engineer, land|1989-02-28|c1d9a7ddb1e34639f...|27.630592| -82.30889|       0|        33598|         1325376416443|1325376416569264|DEUTUS33TRF|\n",
      "|        13|  2019-01-01 00:07:27|   5559857416065248|     fraud_Kiehn Inc|  grocery_pos| 96.29|           Jack@Hill|            NULL|     M|5916 Susan Bridge...|             Grenada|   CA|96038|41.6125|-122.5258|     589|     Systems analyst|1945-12-21|413636e759663f264...| 41.65752|-122.23035|       0|         NULL|         1325376447771|1325376447442465|   ACEEUS31|\n",
      "|        14|  2019-01-01 00:09:03|   3514865930894695|   fraud_Beier-Hyatt| shopping_pos|  7.77|Christopher@Casta...|            NULL|     M|1632 Cohen Drive ...|High Rolls Mounta...|   NM|88325|32.9396|-105.8189|     899|     Naval architect|1967-08-30|8a6293af5ed278dea...| 32.86326| -106.5202|       0|         NULL|         1325376543282| 132537654399153|   ACEEUS31|\n",
      "|        15|  2019-01-01 00:09:20|   6011999606625827|fraud_Schmidt and...| shopping_net|  3.26|       Ronald@Carson|            NULL|     M|     870 Rocha Drive|     Harrington Park|   NJ| 7640|40.9918|   -73.98|    4664|Radiographer, dia...|1965-06-30|baae0b096835c9758...|41.831173|-74.335556|       0|        12446|         1325376560435|1325376560448213|CITIUS33CHI|\n",
      "|        16|  2019-01-01 00:10:49|   6011860238257910|fraud_Lebsack and...|     misc_net| 327.0|                Lisa|          Mendez|     F|44259 Beth Statio...|              Lahoma|   OK|73754| 36.385| -98.0727|    1078|Programme researc...|1952-07-06|991c04803b4d4eeab...| 36.38409| -99.04847|       0|        73852|         1325376649369| 132537664977977|CITIUS33CHI|\n",
      "|        17|  2019-01-01 00:10:58|   3565423334076143|  fraud_Mayert Group| shopping_pos|341.67|              Nathan|    Thomas,eeeee|     M|4923 Campbell Pin...|            Carlisle|   IN|47838|38.9763| -87.3667|    4081|     Energy engineer|1938-03-15|f12cf52be2175703d...| 38.67449| -88.30576|       0|        62824|         1325376658125|1325376658780653|       NULL|\n",
      "|        18|  2019-01-01 00:11:14|   2348245054386329|fraud_Konopelski,...|  food_dining| 63.07|              Justin|             Gay|     M|268 Hayes Rue Sui...|         Harborcreek|   PA|16421|42.1767| -79.9416|    2518|     Event organiser|1946-02-02|8500f3d459047eac8...|41.430275| -79.49255|       0|        16364|         1325376674800|1325376674645606|   APBCUS61|\n",
      "|        19|  2019-01-01 00:12:34|4956828990005111019|fraud_Schultz, Si...|  grocery_pos| 44.71|             Kenneth|, Robinson NOOOO|     M|  269 Sanchez Rapids|           Elizabeth|   NJ| 7208|40.6747| -74.2239|  124967|Operational resea...|1980-12-21|09eff9c806365e2a6...| 40.07959| -74.84808|       0|        08016|         1325376754863| 132537675433224|       NULL|\n",
      "+----------+---------------------+-------------------+--------------------+-------------+------+--------------------+----------------+------+--------------------+--------------------+-----+-----+-------+---------+--------+--------------------+----------+--------------------+---------+----------+--------+-------------+----------------------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "root\n",
      " |-- Unnamed: 0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: string (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: float (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- long: float (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- merch_lat: float (nullable = true)\n",
      " |-- merch_long: float (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- merch_zipcode: string (nullable = true)\n",
      " |-- merch_last_update_time: long (nullable = true)\n",
      " |-- merch_eff_time: long (nullable = true)\n",
      " |-- cc_bic: string (nullable = true)\n",
      "\n",
      "Checking for conversion failures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:========================>                                 (3 + 4) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+----------+--------+-------------+----------------------+--------------+------+\n",
      "|Unnamed: 0|trans_date_trans_time|cc_num|merchant|category|amt|first|last|gender|street|city|state|zip|lat|long|city_pop|job|dob|trans_num|merch_lat|merch_long|is_fraud|merch_zipcode|merch_last_update_time|merch_eff_time|cc_bic|\n",
      "+----------+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+----------+--------+-------------+----------------------+--------------+------+\n",
      "+----------+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+----------+--------+-------------+----------------------+--------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# Read the JSON data from the file\n",
    "df = spark.read.json(path)\n",
    "\n",
    "def clean_json_string(json_str):\n",
    "    \"\"\"\n",
    "    Clean JSON string by:\n",
    "    1. Removing all backslashes\n",
    "    2. Removing quotes around JSON objects (e.g., \"{ }\" becomes { })\n",
    "    \"\"\"\n",
    "    if json_str is None:\n",
    "        return None\n",
    "    \n",
    "    # Remove all backslashes\n",
    "    cleaned = json_str.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    # Remove quotes around JSON objects - pattern: \"{ ... }\"\n",
    "    # This regex finds quoted JSON objects and removes the outer quotes\n",
    "    cleaned = re.sub(r'\"\\s*\\{\\s*(.*?)\\s*\\}\\s*\"', r'{\\1}', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Register UDF\n",
    "clean_json_udf = udf(clean_json_string, StringType())\n",
    "\n",
    "# Apply cleaning to the personal_detail column\n",
    "df_cleaned = df.withColumn(\"personal_detail\", clean_json_udf(sf.col(\"personal_detail\")))\n",
    "\n",
    "# Define schema for address (nested within personal_detail) - all strings initially\n",
    "address_schema = StructType([\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for personal_detail - all strings initially\n",
    "personal_schema = StructType([\n",
    "    StructField(\"person_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address\", address_schema, True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse the cleaned JSON string into proper columns (overwrite the original column)\n",
    "df_with_parsed_personal = df_cleaned.withColumn(\"personal_detail\", sf.from_json(sf.col(\"personal_detail\"), personal_schema))\n",
    "\n",
    "\n",
    "\n",
    "df_with_names = df_with_parsed_personal \\\n",
    "    .withColumn(\"name_parts\", sf.split(sf.col(\"personal_detail.person_name\"), \",\")) \\\n",
    "    .withColumn(\"first\", \n",
    "                # Get the first element if the array is not empty\n",
    "                sf.when(sf.size(sf.col(\"name_parts\")) >= 1, sf.trim(sf.element_at(sf.col(\"name_parts\"), 1)))\n",
    "                .otherwise(None)) \\\n",
    "    .withColumn(\"last\", \n",
    "                # If there's more than one part, concatenate from the second part onwards\n",
    "                sf.when(sf.size(sf.col(\"name_parts\")) >= 2, \n",
    "                        sf.trim(sf.concat_ws(\",\", sf.slice(sf.col(\"name_parts\"), 2, sf.size(sf.col(\"name_parts\"))))))\n",
    "                .otherwise(None)) \\\n",
    "    .drop(\"name_parts\") # Drop the intermediate column used for splitting\n",
    "\n",
    "\n",
    "# Flatten the personal_detail structure and address structure\n",
    "df_flattened = df_with_names.select(\n",
    "    # Original columns in desired order\n",
    "    sf.col(\"Unnamed: 0\"),\n",
    "    sf.col(\"trans_date_trans_time\"),\n",
    "    sf.col(\"cc_num\"),\n",
    "    sf.col(\"merchant\"),\n",
    "    sf.col(\"category\"),\n",
    "    sf.col(\"amt\"),\n",
    "    \n",
    "    sf.col(\"first\"),\n",
    "    sf.col(\"last\"),\n",
    "\n",
    "    # Personal details\n",
    "    sf.col(\"personal_detail.gender\").alias(\"gender\"),\n",
    "    \n",
    "    # Flattened address details\n",
    "    sf.col(\"personal_detail.address.street\").alias(\"street\"),\n",
    "    sf.col(\"personal_detail.address.city\").alias(\"city\"),\n",
    "    sf.col(\"personal_detail.address.state\").alias(\"state\"),\n",
    "    sf.col(\"personal_detail.address.zip\").alias(\"zip\"),\n",
    "    \n",
    "    # Location and demographic info\n",
    "    sf.col(\"personal_detail.lat\").alias(\"lat\"),\n",
    "    sf.col(\"personal_detail.long\").alias(\"long\"),\n",
    "    sf.col(\"personal_detail.city_pop\").alias(\"city_pop\"),\n",
    "    sf.col(\"personal_detail.job\").alias(\"job\"),\n",
    "    sf.col(\"personal_detail.dob\").alias(\"dob\"),\n",
    "    \n",
    "    # Transaction details\n",
    "    sf.col(\"trans_num\"),\n",
    "    sf.col(\"merch_lat\"),\n",
    "    sf.col(\"merch_long\"),\n",
    "    sf.col(\"is_fraud\"),\n",
    "    sf.col(\"merch_zipcode\"),\n",
    "    sf.col(\"merch_last_update_time\"),\n",
    "    sf.col(\"merch_eff_time\"),\n",
    "    sf.col(\"cc_bic\")\n",
    ")\n",
    "\n",
    "# Type conversions and rounding in one operation\n",
    "df_final = df_flattened.withColumns({\n",
    "    'Unnamed: 0': sf.col(\"Unnamed: 0\").cast(IntegerType()),\n",
    "    'trans_date_trans_time': sf.to_timestamp(sf.col(\"trans_date_trans_time\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "    'amt': sf.round(sf.col(\"amt\").cast(FloatType()), 6),\n",
    "    'merch_lat': sf.round(sf.col(\"merch_lat\").cast(FloatType()), 6),\n",
    "    'merch_long': sf.round(sf.col(\"merch_long\").cast(FloatType()), 6),\n",
    "    'is_fraud': sf.col(\"is_fraud\").cast(IntegerType()),\n",
    "    'merch_eff_time': sf.col(\"merch_eff_time\").cast(LongType()),\n",
    "    'merch_last_update_time': sf.col(\"merch_last_update_time\").cast(LongType()),\n",
    "    'lat': sf.round(sf.col(\"lat\").cast(FloatType()), 6),\n",
    "    'long': sf.round(sf.col(\"long\").cast(FloatType()), 6),\n",
    "    'city_pop': sf.col(\"city_pop\").cast(IntegerType())\n",
    "})\n",
    "\n",
    "# Handle null values and \"NA\" strings for all string columns automatically\n",
    "string_columns = [field.name for field in df_final.schema.fields if field.dataType.typeName() == 'string']\n",
    "\n",
    "# Create dictionary for null value handling across all string columns\n",
    "null_handling_dict = {}\n",
    "for col_name in string_columns:\n",
    "    null_handling_dict[col_name] = sf.when(\n",
    "        (sf.lower(sf.col(col_name)) == \"na\") | \n",
    "        (sf.lower(sf.col(col_name)) == \"null\") | \n",
    "        (sf.col(col_name) == \"\"), \n",
    "        None\n",
    "    ).otherwise(sf.col(col_name))\n",
    "\n",
    "df_final = df_final.withColumns(null_handling_dict)\n",
    "\n",
    "# Show final result\n",
    "df_final.show()\n",
    "\n",
    "# Show schema to verify structure\n",
    "df_final.printSchema()\n",
    "\n",
    "# Optional: Show any conversion failures\n",
    "print(\"Checking for conversion failures...\")\n",
    "df_final.filter(sf.col(\"amt\").isNull() | sf.col(\"lat\").isNull() | sf.col(\"city_pop\").isNull()).show()\n",
    "\n",
    "# Save the cleaned data\n",
    "# df_final.write.mode(\"overwrite\").json(\"cleaned_output_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0828817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:15:41 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 101) \n",
      "org.apache.spark.SparkArrayIndexOutOfBoundsException: [INVALID_ARRAY_INDEX] The index 1 is out of bounds. The array has 1 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. SQLSTATE: 22003\n",
      "== DataFrame ==\n",
      "\"__getitem__\" was called from\n",
      "line 65 in cell [10]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidArrayIndexError(QueryExecutionErrors.scala:233)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidArrayIndexError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/07/21 01:15:41 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 101) (10.255.255.254 executor driver): org.apache.spark.SparkArrayIndexOutOfBoundsException: [INVALID_ARRAY_INDEX] The index 1 is out of bounds. The array has 1 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. SQLSTATE: 22003\n",
      "== DataFrame ==\n",
      "\"__getitem__\" was called from\n",
      "line 65 in cell [10]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidArrayIndexError(QueryExecutionErrors.scala:233)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidArrayIndexError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/21 01:15:41 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job\n",
      "{\"ts\": \"2025-07-21 01:15:41.638\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[INVALID_ARRAY_INDEX] The index 1 is out of bounds. The array has 1 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. SQLSTATE: 22003\", \"context\": {\"file\": \"line 65 in cell [10]\", \"line\": \"\", \"fragment\": \"__getitem__\", \"errorClass\": \"INVALID_ARRAY_INDEX\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o3277.showString.\\n: org.apache.spark.SparkArrayIndexOutOfBoundsException: [INVALID_ARRAY_INDEX] The index 1 is out of bounds. The array has 1 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. SQLSTATE: 22003\\n== DataFrame ==\\n\\\"__getitem__\\\" was called from\\nline 65 in cell [10]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidArrayIndexError(QueryExecutionErrors.scala:233)\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidArrayIndexError(QueryExecutionErrors.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\\n\\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\\n\\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\\n\\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/jeevin/miniconda3/envs/paynet-test/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/jeevin/miniconda3/envs/paynet-test/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "ArrayIndexOutOfBoundsException",
     "evalue": "[INVALID_ARRAY_INDEX] The index 1 is out of bounds. The array has 1 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. SQLSTATE: 22003\n== DataFrame ==\n\"__getitem__\" was called from\nline 65 in cell [10]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrayIndexOutOfBoundsException\u001b[39m            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    123\u001b[39m df_final = df_final.withColumns(null_handling_dict)\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Show final result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Show schema to verify structure\u001b[39;00m\n\u001b[32m    129\u001b[39m df_final.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/paynet-test/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/paynet-test/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/paynet-test/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/paynet-test/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mArrayIndexOutOfBoundsException\u001b[39m: [INVALID_ARRAY_INDEX] The index 1 is out of bounds. The array has 1 elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. SQLSTATE: 22003\n== DataFrame ==\n\"__getitem__\" was called from\nline 65 in cell [10]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"JSONCleaner\").getOrCreate()\n",
    "\n",
    "def clean_json_string(json_str):\n",
    "    \"\"\"\n",
    "    Clean JSON string by:\n",
    "    1. Removing all backslashes\n",
    "    2. Removing quotes around JSON objects (e.g., \"{ }\" becomes { })\n",
    "    \"\"\"\n",
    "    if json_str is None:\n",
    "        return None\n",
    "    \n",
    "    # Remove all backslashes\n",
    "    cleaned = json_str.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    # Remove quotes around JSON objects - pattern: \"{ ... }\"\n",
    "    # This regex finds quoted JSON objects and removes the outer quotes\n",
    "    cleaned = re.sub(r'\"\\s*\\{\\s*(.*?)\\s*\\}\\s*\"', r'{\\1}', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Register UDF\n",
    "clean_json_udf = udf(clean_json_string, StringType())\n",
    "\n",
    "# Apply cleaning to the personal_detail column\n",
    "df_cleaned = df.withColumn(\"personal_detail\", clean_json_udf(col(\"personal_detail\")))\n",
    "\n",
    "# Define schema for address (nested within personal_detail) - all strings initially\n",
    "address_schema = StructType([\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for personal_detail - all strings initially\n",
    "personal_schema = StructType([\n",
    "    StructField(\"person_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address\", address_schema, True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse the cleaned JSON string into proper columns (overwrite the original column)\n",
    "df_with_parsed_personal = df_cleaned.withColumn(\"personal_detail\", from_json(col(\"personal_detail\"), personal_schema))\n",
    "\n",
    "# Flatten the personal_detail structure and address structure\n",
    "df_flattened = df_with_parsed_personal.select(\n",
    "    # Original columns in desired order\n",
    "    col(\"Unnamed: 0\"),\n",
    "    col(\"trans_date_trans_time\"),\n",
    "    col(\"cc_num\"),\n",
    "    col(\"merchant\"),\n",
    "    col(\"category\"),\n",
    "    col(\"amt\"),\n",
    "    \n",
    "    # Split person_name into first and last names\n",
    "    split(col(\"personal_detail.person_name\"), \",\")[0].alias(\"first\"),\n",
    "    split(col(\"personal_detail.person_name\"), \",\")[1].alias(\"last\"),\n",
    "    \n",
    "    # Personal details\n",
    "    col(\"personal_detail.gender\").alias(\"gender\"),\n",
    "    \n",
    "    # Flattened address details\n",
    "    col(\"personal_detail.address.street\").alias(\"street\"),\n",
    "    col(\"personal_detail.address.city\").alias(\"city\"),\n",
    "    col(\"personal_detail.address.state\").alias(\"state\"),\n",
    "    col(\"personal_detail.address.zip\").alias(\"zip\"),\n",
    "    \n",
    "    # Location and demographic info\n",
    "    col(\"personal_detail.lat\").alias(\"lat\"),\n",
    "    col(\"personal_detail.long\").alias(\"long\"),\n",
    "    col(\"personal_detail.city_pop\").alias(\"city_pop\"),\n",
    "    col(\"personal_detail.job\").alias(\"job\"),\n",
    "    col(\"personal_detail.dob\").alias(\"dob\"),\n",
    "    \n",
    "    # Transaction details\n",
    "    col(\"trans_num\"),\n",
    "    col(\"merch_lat\"),\n",
    "    col(\"merch_long\"),\n",
    "    col(\"is_fraud\"),\n",
    "    col(\"merch_zipcode\"),\n",
    "    col(\"merch_last_update_time\"),\n",
    "    col(\"merch_eff_time\"),\n",
    "    col(\"cc_bic\")\n",
    ")\n",
    "\n",
    "\n",
    "# Type conversions and rounding in one operation\n",
    "df_final = df_flattened.withColumns({\n",
    "    'Unnamed: 0': col(\"Unnamed: 0\").cast(IntegerType()),\n",
    "    'trans_date_trans_time': to_timestamp(col(\"trans_date_trans_time\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "    'amt': sp_round(col(\"amt\").cast(FloatType()), 6),\n",
    "    'merch_lat': sp_round(col(\"merch_lat\").cast(FloatType()), 6),\n",
    "    'merch_long': sp_round(col(\"merch_long\").cast(FloatType()), 6),\n",
    "    'is_fraud': col(\"is_fraud\").cast(IntegerType()),\n",
    "    'merch_eff_time': col(\"merch_eff_time\").cast(LongType()),\n",
    "    'merch_last_update_time': col(\"merch_last_update_time\").cast(LongType()),\n",
    "    'lat': sp_round(col(\"lat\").cast(FloatType()), 6),\n",
    "    'long': sp_round(col(\"long\").cast(FloatType()), 6),\n",
    "    'city_pop': col(\"city_pop\").cast(IntegerType())\n",
    "})\n",
    "\n",
    "# Handle null values and \"NA\" strings for all string columns automatically\n",
    "string_columns = [field.name for field in df_final.schema.fields if field.dataType.typeName() == 'string']\n",
    "\n",
    "# Create dictionary for null value handling across all string columns\n",
    "null_handling_dict = {}\n",
    "for col_name in string_columns:\n",
    "    null_handling_dict[col_name] = when(\n",
    "        (lower(col(col_name)) == \"na\") | \n",
    "        (lower(col(col_name)) == \"null\") | \n",
    "        (col(col_name) == \"\"), \n",
    "        None\n",
    "    ).otherwise(col(col_name))\n",
    "\n",
    "df_final = df_final.withColumns(null_handling_dict)\n",
    "\n",
    "# Show final result\n",
    "df_final.show()\n",
    "\n",
    "# Show schema to verify structure\n",
    "df_final.printSchema()\n",
    "\n",
    "# Optional: Show any conversion failures\n",
    "print(\"Checking for conversion failures...\")\n",
    "df_final.filter(col(\"amt\").isNull() | col(\"lat\").isNull() | col(\"city_pop\").isNull()).show()\n",
    "\n",
    "# Save the cleaned data\n",
    "# df_final.write.mode(\"overwrite\").json(\"cleaned_output_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98cebf",
   "metadata": {},
   "source": [
    "Infer insights from the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paynet-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
