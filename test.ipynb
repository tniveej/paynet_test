{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30de7ff6",
   "metadata": {},
   "source": [
    "Importing Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kagglehub import dataset_download\n",
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, TimestampType, LongType\n",
    "import re\n",
    "\n",
    "path: str = dataset_download(\"jinquan/cc-sample-data\")\n",
    "\n",
    "print(path)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"payNet\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67be3f",
   "metadata": {},
   "source": [
    "Load JSON and clean up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95900d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON data from the file\n",
    "df = spark.read.json(path)\n",
    "\n",
    "def clean_json_string(json_str):\n",
    "    \"\"\"\n",
    "    Clean JSON string by:\n",
    "    1. Removing all backslashes\n",
    "    2. Removing quotes around JSON objects (e.g., \"{ }\" becomes { })\n",
    "    \"\"\"\n",
    "    if json_str is None:\n",
    "        return None\n",
    "    \n",
    "    # Remove all backslashes\n",
    "    cleaned = json_str.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    # Remove quotes around JSON objects - pattern: \"{ ... }\"\n",
    "    # This regex finds quoted JSON objects and removes the outer quotes\n",
    "    cleaned = re.sub(r'\"\\s*\\{\\s*(.*?)\\s*\\}\\s*\"', r'{\\1}', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Register UDF\n",
    "clean_json_udf = sf.udf(clean_json_string, StringType())\n",
    "\n",
    "# Apply cleaning to the personal_detail column\n",
    "df = df.withColumn(\"personal_detail\", clean_json_udf(sf.col(\"personal_detail\")))\n",
    "\n",
    "# Define schema for address (nested within personal_detail) - all strings initially\n",
    "address_schema = StructType([\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for personal_detail - all strings initially\n",
    "personal_schema = StructType([\n",
    "    StructField(\"person_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address\", address_schema, True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse the cleaned JSON string into proper columns (overwrite the original column)\n",
    "df_with_parsed_personal = df.withColumn(\"personal_detail\", sf.from_json(sf.col(\"personal_detail\"), personal_schema))\n",
    "\n",
    "# for debugging\n",
    "# df_with_parsed_personal.select(col(\"personal_detail.person_name\")).show()\n",
    "# df_with_parsed_personal.select(col(\"personal_detail.address.street\")).show()\n",
    "\n",
    "# Process name splitting for 'first' and 'last' names with improved robustness.\n",
    "df_with_names = df_with_parsed_personal.withColumn(\n",
    "    \"cleaned_person_name\",\n",
    "    sf.when(sf.col(\"personal_detail.person_name\").isNotNull(),\n",
    "        sf.trim(\n",
    "            sf.regexp_replace( # Normalize multiple spaces to single space\n",
    "                sf.regexp_replace( # Replace all non-alphanumeric characters (except spaces) with a single space\n",
    "                    sf.regexp_replace( # Remove specific trailing strings like 'eeeee' and 'N' followed by 4 or more '0' or 'O' (case-insensitive)\n",
    "                        sf.regexp_replace(sf.col(\"personal_detail.person_name\"), r\"(?i),?eeeee$\", \"\"),\n",
    "                        r\"(?i),?\\s*N[0O]{4,}$\", \"\" # Updated regex to handle N0000, NOOOO etc.\n",
    "                    ),\n",
    "                    r\"[^a-zA-Z0-9\\s]\", \" \" # Replace any character that is NOT a letter, number, or whitespace with a space. This will catch /, !, @, |, and also the comma if it's not part of a \"Lastname, Firstname\" pattern.\n",
    "                ),\n",
    "                r\"\\s+\", \" \" # Normalize multiple spaces to single space\n",
    "            )\n",
    "        )\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_with_names = df_with_names \\\n",
    "    .withColumn(\"name_parts\", sf.split(sf.col(\"cleaned_person_name\"), \" \")) \\\n",
    "    .withColumn(\"first\", \n",
    "        sf.when(sf.size(sf.col(\"name_parts\")) >= 1, sf.trim(sf.element_at(sf.col(\"name_parts\"), 1)))\n",
    "        .otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\"last\", \n",
    "        sf.when(sf.size(sf.col(\"name_parts\")) > 1, \n",
    "                sf.trim(sf.concat_ws(\" \", sf.slice(sf.col(\"name_parts\"), 2, sf.size(sf.col(\"name_parts\"))))))\n",
    "        .otherwise(None)\n",
    "    ) \\\n",
    "    .drop(\"cleaned_person_name\", \"name_parts\") # Drop intermediate columns\n",
    "\n",
    "\n",
    "# Flatten the personal_detail structure and address structure\n",
    "df_flattened = df_with_names.select(\n",
    "    # Original columns in desired order\n",
    "    sf.col(\"Unnamed: 0\"),\n",
    "    sf.col(\"trans_date_trans_time\"),\n",
    "    sf.col(\"cc_num\"),\n",
    "    sf.col(\"merchant\"),\n",
    "    sf.col(\"category\"),\n",
    "    sf.col(\"amt\"),\n",
    "    \n",
    "    sf.col(\"first\"),\n",
    "    sf.col(\"last\"),\n",
    "\n",
    "    # Personal details\n",
    "    sf.col(\"personal_detail.gender\").alias(\"gender\"),\n",
    "    \n",
    "    # Flattened address details\n",
    "    sf.col(\"personal_detail.address.street\").alias(\"street\"),\n",
    "    sf.col(\"personal_detail.address.city\").alias(\"city\"),\n",
    "    sf.col(\"personal_detail.address.state\").alias(\"state\"),\n",
    "    sf.col(\"personal_detail.address.zip\").alias(\"zip\"),\n",
    "    \n",
    "    # Location and demographic info\n",
    "    sf.col(\"personal_detail.lat\").alias(\"lat\"),\n",
    "    sf.col(\"personal_detail.long\").alias(\"long\"),\n",
    "    sf.col(\"personal_detail.city_pop\").alias(\"city_pop\"),\n",
    "    sf.col(\"personal_detail.job\").alias(\"job\"),\n",
    "    sf.col(\"personal_detail.dob\").alias(\"dob\"),\n",
    "    \n",
    "    # Transaction details\n",
    "    sf.col(\"trans_num\"),\n",
    "    sf.col(\"merch_lat\"),\n",
    "    sf.col(\"merch_long\"),\n",
    "    sf.col(\"is_fraud\"),\n",
    "    sf.col(\"merch_zipcode\"),\n",
    "    sf.col(\"merch_last_update_time\"),\n",
    "    sf.col(\"merch_eff_time\"),\n",
    "    sf.col(\"cc_bic\")\n",
    ")\n",
    "\n",
    "\n",
    "# Type conversions and rounding in one operation, including date format and timezone handling. Assuming initial timzeone is UTC and converting to UTC\n",
    "df_cleaned = df_flattened.withColumns({\n",
    "    'Unnamed: 0': sf.col(\"Unnamed: 0\").cast(IntegerType()),\n",
    "\n",
    "\n",
    "    # Convert trans_date_trans_time to TimestampType, then to UTC+8, then format\n",
    "    'trans_date_trans_time': sf.date_format(sf.from_utc_timestamp(sf.col(\"trans_date_trans_time\").cast(\"timestamp\"), \"UTC+8\"), 'yyyy-MM-dd HH:mm:ss Z'),\n",
    "\n",
    "\n",
    "    'amt': sf.round(sf.col(\"amt\").cast(FloatType()), 6),\n",
    "    'merch_lat': sf.round(sf.col(\"merch_lat\").cast(FloatType()), 6),\n",
    "    'merch_long': sf.round(sf.col(\"merch_long\").cast(FloatType()), 6),\n",
    "    'is_fraud': sf.col(\"is_fraud\").cast(IntegerType()),\n",
    "\n",
    "\n",
    "    # Convert merch_eff_time (microseconds) to TimestampType, then to UTC+8, then format\n",
    " \n",
    "    'merch_eff_time': sf.date_format(\n",
    "        sf.from_utc_timestamp(\n",
    "            (\n",
    "                sf.rpad(\n",
    "                    sf.col(\"merch_eff_time\").cast(StringType()),\n",
    "                    16,\n",
    "                    '0'\n",
    "                ).cast(LongType()) / 1000000\n",
    "            ).cast(\"timestamp\"),\n",
    "            \"UTC+8\"\n",
    "        ),\n",
    "        'yyyy-MM-dd HH:mm:ss.SSSSSS Z'\n",
    "    ),\n",
    "\n",
    "\n",
    "    # Convert merch_last_update_time (microseconds) to TimestampType, then to UTC+8, then format\n",
    "    'merch_last_update_time': sf.date_format(\n",
    "        sf.from_utc_timestamp(\n",
    "            (\n",
    "                sf.rpad(\n",
    "                    sf.col(\"merch_last_update_time\").cast(StringType()),\n",
    "                    16,\n",
    "                    '0'\n",
    "                ).cast(LongType()) / 1000000\n",
    "            ).cast(\"timestamp\"),\n",
    "            \"UTC+8\"\n",
    "            ),\n",
    "            'yyyy-MM-dd HH:mm:ss.SSSSSS Z'\n",
    "        ), \n",
    "    \n",
    "    'lat': sf.round(sf.col(\"lat\").cast(FloatType()), 6),\n",
    "    'long': sf.round(sf.col(\"long\").cast(FloatType()), 6),\n",
    "    'city_pop': sf.col(\"city_pop\").cast(IntegerType())\n",
    "})\n",
    "\n",
    "\n",
    "# Handle null values and \"NA\" strings for all string columns automatically\n",
    "string_columns = [field.name for field in df_cleaned.schema.fields if field.dataType.typeName() == 'string']\n",
    "\n",
    "# Create dictionary for null value handling across all string columns\n",
    "null_handling_dict = {}\n",
    "for col_name in string_columns:\n",
    "    null_handling_dict[col_name] = sf.when(\n",
    "        (sf.lower(sf.col(col_name)) == \"na\") | \n",
    "        (sf.lower(sf.col(col_name)) == \"null\") | \n",
    "        (sf.col(col_name) == \"\"), \n",
    "        None\n",
    "    ).otherwise(sf.col(col_name))\n",
    "\n",
    "df_cleaned = df_cleaned.withColumns(null_handling_dict)\n",
    "\n",
    "#all merchant names start with the word 'fraud_'; safe to remove\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"merchant\",\n",
    "    sf.regexp_replace(sf.col(\"merchant\"), \"fraud_\", \"\")\n",
    ")\n",
    "\n",
    "## Display cleaned data \n",
    "\n",
    "# Show final result\n",
    "df_cleaned.show(40,truncate=False)\n",
    "\n",
    "#convert subsection to markdown for viewing\n",
    "# print(df_cleaned.filter(sf.col(\"Unnamed: 0\") <= 25).toPandas().to_markdown())\n",
    "\n",
    "# Show schema to verify structure\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5290c83",
   "metadata": {},
   "source": [
    "Handling PII data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b63b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Identifiers\n",
    "# cc_num, first, last, trans_num, dob\n",
    "\n",
    "# Indirect Identifiers\n",
    "# city, state, zip, street, job, lat, long, cc_bic\n",
    "\n",
    "# Indentifiers to delete : \n",
    "identifier_delete = [\n",
    "    # not needed for analysis\n",
    "    \"cc_num\",\n",
    "    \"first\",\n",
    "    \"last\",\n",
    "    \"trans_num\",\n",
    "    \"cc_bic\",\n",
    "    # specific location not required\n",
    "    \"street\",\n",
    "]\n",
    "\n",
    "# reducing specificity of location by rounding latitude and longitude\n",
    "df_cleaned = df_cleaned.withColumn(\"lat\", sf.round(sf.col(\"lat\"), 2))\n",
    "df_cleaned = df_cleaned.withColumn(\"long\", sf.round(sf.col(\"long\"), 2))\n",
    "\n",
    "# remove columns \n",
    "df_cleaned = df_cleaned.drop(*identifier_delete)\n",
    "\n",
    "# keep only the year (first 4 digits) from dob\n",
    "df_cleaned = df_cleaned.withColumn(\"dob\", sf.substring(sf.col(\"dob\"), 1, 4))\n",
    "\n",
    "df_cleaned.show(40,truncate=False)\n",
    "print(df_cleaned.tail(40))\n",
    "\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22154d14",
   "metadata": {},
   "source": [
    "Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e6ec487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataframe shape: (1296675, 3)\n",
      "\n",
      "Folium map saved to assets/folium_heatmap.html\n",
      "\n",
      "Running Dash application inline in Jupyter Notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7d120fbaef00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dash\n",
    "from dash import html\n",
    "from dash import dcc\n",
    "import plotly.express as px\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "pandas_df = df_cleaned.select([\"lat\", \"long\", \"amt\"]).toPandas()\n",
    "\n",
    "print(f'Pandas dataframe shape: {pandas_df.shape}')\n",
    "\n",
    "heat_data = pandas_df.to_numpy().tolist()\n",
    "\n",
    "if not pandas_df.empty:\n",
    "    map_center = [pandas_df.lat.mean(), pandas_df.long.mean()]\n",
    "else:\n",
    "    map_center = [0, 0]\n",
    "\n",
    "# Set the desired initial zoom level for the Folium map\n",
    "desired_zoom_level = 7\n",
    "m = folium.Map(location=map_center, zoom_start=desired_zoom_level, tiles=\"OpenStreetMap\")\n",
    "\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "assets_folder = 'assets'\n",
    "if not os.path.exists(assets_folder):\n",
    "    os.makedirs(assets_folder)\n",
    "folium_map_path = os.path.join(assets_folder, 'folium_heatmap.html')\n",
    "m.save(folium_map_path)\n",
    "print(f\"\\nFolium map saved to {folium_map_path}\")\n",
    "\n",
    "# Initialize Dash for inline display in notebooks (Dash 2.11+ compatible)\n",
    "app = dash.Dash(__name__) \n",
    "\n",
    "# Define the Dash app layout\n",
    "app.layout = html.Div(\n",
    "    style={'fontFamily': 'Arial, sans-serif', 'padding': '20px', 'backgroundColor': '#f0f2f5'},\n",
    "    children=[\n",
    "        html.H1(\"Data Dashboard\",\n",
    "                style={'textAlign': 'center', 'color': '#333', 'marginBottom': '30px'}),\n",
    "\n",
    "        html.Div(\n",
    "            style={'display': 'flex', 'flexDirection': 'row', 'gap': '20px'},\n",
    "            children=[\n",
    "                # Left panel for the Folium Map\n",
    "                html.Div(\n",
    "                    style={'display':'flex','flexDirection': 'column','justify_content': 'center','align_items': 'center','flex': '1', 'backgroundColor': 'white', 'padding': '20px', 'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'},\n",
    "                    children=[\n",
    "                        html.H2(\"Expenditure Heatmap\", style={'textAlign': 'center', 'color': '#555'}),\n",
    "                        # Use an Iframe to embed the Folium HTML map\n",
    "                        html.Iframe(\n",
    "                            id='folium-map-iframe',\n",
    "                            srcDoc=open(folium_map_path, 'r').read(), # Read the HTML content directly\n",
    "                            style={'width': '100%','height': '400px', 'border': 'none', 'borderRadius': '4px'}\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nRunning Dash application inline in Jupyter Notebook\")\n",
    "app.run(mode='inline', port=8050) # Using mode='inline' for Jupyter Notebook display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paynet-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
